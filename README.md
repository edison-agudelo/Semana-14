# üß† Aprendizaje por Refuerzo con Flask ‚Äì Proyecto Acad√©mico

Este proyecto implementa un entorno interactivo de **Aprendizaje por Refuerzo (Reinforcement Learning, RL)** utilizando **Flask**, **Q-Learning** y un entorno personalizado tipo **GridWorld 5x5**.

Incluye:

- Interfaz web moderna (Bootstrap + dise√±o minimalista)
- Entrenamiento configurable con Q-Learning
- Gr√°fica din√°mica de recompensas por episodio
- Visualizaci√≥n de trayectoria del agente
- S√≠ntesis te√≥rica completa (conceptos, algoritmos, APA7)
- Estructura adecuada para repositorio acad√©mico

---

# üéØ Objetivo del Proyecto

Comprender los fundamentos del Aprendizaje por Refuerzo y aplicarlos mediante la implementaci√≥n de un agente capaz de aprender a tomar decisiones secuenciales. El proyecto permite:

- Definir un entorno RL simple.
- Configurear par√°metros clave de aprendizaje.
- Entrenar un agente mediante Q-Learning.
- Observar las recompensas acumuladas.
- Visualizar la pol√≠tica aprendida.
- Integrar todo en una interfaz Flask.

---

# üìå Contenido del Proyecto

## 1. Conceptos B√°sicos

Incluye teor√≠a sobre:

- Qu√© es RL
- Comparaci√≥n con supervisado y no supervisado
- Componentes: agente, entorno, estados, acciones, recompensas, pol√≠tica
- Explorar vs explotar (Œµ-greedy)
- Retorno acumulado y descuento temporal
- Algoritmos principales:
  - Q-Learning
  - SARSA
  - Deep Q-Network (DQN)
- Buenas pr√°cticas:
  - Manejo de recompensas
  - Estabilidad del entrenamiento
  - Convergencia
  - Exploraci√≥n adecuada

Se incluyen referencias APA 7.

---

## 2. Caso Pr√°ctico ‚Äì GridWorld con Q-Learning

El entorno consta de:

- Grid 5x5
- Estado inicial: (0,0)
- Meta: (4,4)
- Obst√°culos
- Recompensas:
  - -1 por movimiento
  - +10 al llegar a la meta
  - -10 por caer en obst√°culo

### Par√°metros ajustables en la interfaz:

| Par√°metro | Descripci√≥n |
|----------|-------------|
| `episodes` | N√∫mero de episodios de entrenamiento |
| `max_steps` | M√°x. pasos por episodio |
| `alpha` | Tasa de aprendizaje |
| `gamma` | Factor de descuento |
| `epsilon` | Exploraci√≥n inicial |
| `epsilon_min` | Exploraci√≥n m√≠nima |
| `epsilon_decay` | Disminuci√≥n progresiva de Œµ |

### Resultados generados:

- Archivo `q_table.pkl`
- Gr√°fica din√°mica de recompensas
- Trayectoria del agente usando pol√≠tica greedy

---

# üñ•Ô∏è Interfaz Web Flask

La aplicaci√≥n expone 2 secciones:

### ‚úî Conceptos B√°sicos  
Explicaci√≥n te√≥rica completa (RL, algoritmos, APA).

### ‚úî Caso Pr√°ctico  
Entrenamiento interactivo + visualizaciones:

- Entrenar agente
- Mostrar gr√°fica de recompensas
- Probar pol√≠tica aprendida
- Ver trayectorias en GridWorld

---

# üìÇ Estructura del Proyecto
/static
/templates
base.html
index.html
rl_conceptos.html
rl_caso_practico.html
rl_gridworld.py
app.py
q_table.pkl (generado tras entrenamiento)
README.md
