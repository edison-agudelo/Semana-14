{% extends "base.html" %}
{% block title %}Conceptos bÃ¡sicos{% endblock %}

{% block content %}
<h1>Conceptos BÃ¡sicos del Aprendizaje por Refuerzo (RL)</h1>

<p>
El Aprendizaje por Refuerzo (Reinforcement Learning, RL) es...
<br><br>

 1. SÃ­ntesis teÃ³rica â€“ â€œConceptos bÃ¡sicosâ€ de Aprendizaje por Refuerzo



1.1. DefiniciÃ³n general de Aprendizaje por Refuerzo y diferencia con otros enfoques

El Aprendizaje por Refuerzo (Reinforcement Learning, RL) es un paradigma de aprendizaje automÃ¡tico en el que un agente aprende a tomar decisiones secuenciales interactuando con un entorno. En cada paso de tiempo, el agente observa un estado, selecciona una acciÃ³n y recibe una recompensa (positiva o negativa) junto con un nuevo estado. El objetivo es aprender una polÃ­tica que maximice la suma de recompensas a largo plazo, llamada retorno acumulado. 
Ideas Incompletas

A diferencia del aprendizaje supervisado, donde el modelo aprende a partir de ejemplos etiquetados (entradaâ€“salida correcta), en RL el agente no recibe directamente la â€œrespuesta correctaâ€, sino que descubre por ensayo y error quÃ© acciones conducen a mejores recompensas. El aprendizaje no supervisado, por su parte, busca estructuras o patrones en los datos (por ejemplo, agrupamientos) sin etiquetas ni seÃ±ales de recompensa, mientras que RL se centra en la toma de decisiones en el tiempo bajo retroalimentaciÃ³n escalar (recompensas). 
Ideas Incompletas
+1

1.2. Componentes del modelo RL

Un problema de RL suele modelarse como un Proceso de DecisiÃ³n de Markov (MDP) con los siguientes componentes:

Agente: entidad que aprende y toma decisiones.

Entorno (Environment): â€œmundoâ€ con el que interactÃºa el agente.

Estados (S): representaciÃ³n de la situaciÃ³n actual del entorno.

Acciones (A): conjunto de decisiones que el agente puede tomar en cada estado.

Recompensa (R): seÃ±al numÃ©rica que indica quÃ© tan buena fue la acciÃ³n tomada.

PolÃ­tica (Ï€): regla (determinista o estocÃ¡stica) que define quÃ© acciÃ³n tomar en cada estado.

El agente busca una polÃ­tica Ã³ptima Ï€* que maximice el retorno esperado de recompensas a lo largo del tiempo. 
Ideas Incompletas

1.3. Principios del ciclo de aprendizaje

ExploraciÃ³n vs explotaciÃ³n:

ExploraciÃ³n: probar acciones nuevas para descubrir recompensas potencialmente mejores.

ExplotaciÃ³n: elegir las acciones que ya se sabe que dan buena recompensa.
El equilibrio se suele controlar con una polÃ­tica Îµ-greedy, donde con probabilidad Îµ se explora y con 1âˆ’Îµ se explota.

Retorno acumulado y descuento temporal:
El objetivo es maximizar el retorno acumulado:

ğº
ğ‘¡
=
âˆ‘
ğ‘˜
=
0
âˆ
ğ›¾
ğ‘˜
ğ‘…
ğ‘¡
+
ğ‘˜
+
1
,
G
t
	â€‹

=
k=0
âˆ‘
âˆ
	â€‹

Î³
k
R
t+k+1
	â€‹

,

donde 
0
â‰¤
ğ›¾
â‰¤
1
0â‰¤Î³â‰¤1 es el factor de descuento. Un Î³ cercano a 1 valora las recompensas futuras, mientras que un Î³ bajo favorece las recompensas inmediatas.

ActualizaciÃ³n basada en diferencia temporal (TD):
Muchos algoritmos de RL usan mÃ©todos de Diferencia Temporal (TD), que actualizan los valores de los estados o pares estadoâ€“acciÃ³n usando la diferencia entre una estimaciÃ³n actual y una estimaciÃ³n bootstrap del futuro. 
Ideas Incompletas

1.4. Algoritmos principales y campo de aplicaciÃ³n

Q-Learning:
Algoritmo off-policy que aprende una funciÃ³n valor de acciÃ³n 
ğ‘„
(
ğ‘ 
,
ğ‘
)
Q(s,a) que aproxima el retorno esperado al tomar la acciÃ³n a en el estado s y seguir la polÃ­tica Ã³ptima despuÃ©s. Su regla de actualizaciÃ³n tÃ­pica es:

ğ‘„
(
ğ‘ 
,
ğ‘
)
â†
ğ‘„
(
ğ‘ 
,
ğ‘
)
+
ğ›¼
â€‰
[
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘
â€²
ğ‘„
(
ğ‘ 
â€²
,
ğ‘
â€²
)
âˆ’
ğ‘„
(
ğ‘ 
,
ğ‘
)
]
,
Q(s,a)â†Q(s,a)+Î±[r+Î³
a
â€²
max
	â€‹

Q(s
â€²
,a
â€²
)âˆ’Q(s,a)],

donde Î± es la tasa de aprendizaje. Se usa en problemas con espacio de estados discreto, como entornos tipo GridWorld o tareas simples de control. 
Ideas Incompletas

SARSA (Stateâ€“Actionâ€“Rewardâ€“Stateâ€“Action):
Algoritmo on-policy que actualiza los valores Q usando la acciÃ³n realmente seguida por la polÃ­tica de comportamiento. Es mÃ¡s conservador y a veces mÃ¡s estable en entornos estocÃ¡sticos.

Deep Q-Network (DQN):
Combina RL con redes neuronales profundas para aproximar la funciÃ³n Q cuando los estados son de alta dimensiÃ³n (por ejemplo, imÃ¡genes de videojuegos Atari). Introduce tÃ©cnicas como experience replay y redes objetivo para estabilizar el entrenamiento. DQN alcanzÃ³ desempeÃ±o a nivel humano en varios juegos Atari. 
Nature
+1

Los campos de aplicaciÃ³n incluyen videojuegos, robÃ³tica, sistemas de recomendaciÃ³n, finanzas, salud y control industrial, especialmente cuando las decisiones deben tomarse de manera secuencial en entornos cambiantes. 
Now Publishers

1.5. Buenas prÃ¡cticas en RL

Estabilidad del aprendizaje: usar tasas de aprendizaje moderadas, normalizaciÃ³n de recompensas y, en DRL, tÃ©cnicas como experience replay y redes objetivo. 
arXiv
+1

Tasa de exploraciÃ³n: comenzar con un Îµ alto (mucha exploraciÃ³n) y disminuirlo gradualmente a medida que el agente aprende, evitando tanto la exploraciÃ³n eterna como el estancamiento prematuro.

Manejo de recompensas: diseÃ±ar funciones de recompensa que incentiven el comportamiento deseado (por ejemplo, dar recompensas negativas por acciones peligrosas o por episodios demasiado largos).

Convergencia: monitorear la recompensa promedio por episodio para detectar si el agente deja de mejorar. En tabular Q-Learning, con suficiente exploraciÃ³n y condiciones adecuadas, el algoritmo converge a la polÃ­tica Ã³ptima. 
Ideas Incompletas

GeneralizaciÃ³n: en deep RL, buscar que la polÃ­tica aprenda patrones que funcionen en estados no vistos, evitando sobreajuste al entorno de entrenamiento. 
arXiv
+1

Referencias en formato APA 7 (puedes copiar tal cual)

FranÃ§ois-Lavet, V., Henderson, P., Islam, R., Bellemare, M. G., & Pineau, J. (2018). An introduction to deep reinforcement learning. Foundations and Trends in Machine Learning, 11(3â€“4), 219â€“354. https://doi.org/10.1561/2200000071
 
Now Publishers

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., â€¦ Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529â€“533. https://doi.org/10.1038/nature14236
 
Nature

Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). MIT Press. 
Google Libros
</p>

<h3>Referencias APA</h3>
<ul>
    <li>FranÃ§ois-Lavet, V. et al. (2018)...</li>
    <li>Mnih, V. et al. (2015)...</li>
    <li>Sutton, R. S., & Barto, A. (2018)...</li>
</ul>

{% endblock %}
