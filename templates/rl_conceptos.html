{% extends "base.html" %}
{% block title %}Conceptos b√°sicos{% endblock %}

{% block content %}

<h1 class="mb-4">Conceptos B√°sicos del Aprendizaje por Refuerzo (RL)</h1>

<!-- OBJETIVO -->
<div class="card-custom mb-4">
    <h2>üéØ Objetivo</h2>
    <p>
        Comprender los fundamentos del Aprendizaje por Refuerzo (Reinforcement Learning, RL)
        y aplicarlos mediante la implementaci√≥n de un agente capaz de aprender a interactuar
        con un entorno definido. El objetivo es modelar un proceso de decisi√≥n secuencial,
        entrenar al agente mediante retroalimentaci√≥n basada en recompensas
        y visualizar su desempe√±o mediante m√©tricas y simulaciones dentro de una aplicaci√≥n Flask.
    </p>
</div>

<!-- INSTRUCCIONES -->
<div class="card-custom mb-4">
    <h2>üìå Instrucciones</h2>

    <h4>1. Investigaci√≥n ‚Äì Conceptos b√°sicos</h4>
    <ul>
        <li>Definir qu√© es el Aprendizaje por Refuerzo.</li>
        <li>Diferencias entre aprendizaje supervisado, no supervisado y por refuerzo.</li>
        <li>Explicar los componentes del modelo RL.</li>
        <li>Describir exploraci√≥n‚Äìexplotaci√≥n, retorno acumulado y descuento temporal.</li>
        <li>Explicar Q-Learning, SARSA y Deep Q-Network.</li>
        <li>Incluir buenas pr√°cticas y 2‚Äì3 referencias APA 7.</li>
    </ul>

    <h4>2. Desarrollo ‚Äì Implementaci√≥n del agente</h4>
    <ul>
        <li>Se utiliza un entorno GridWorld 5x5 como ejemplo.</li>
        <li>Definir estados, acciones y funci√≥n de recompensa.</li>
        <li>Implementar un agente Q-Learning.</li>
        <li>Registrar recompensas por episodio.</li>
        <li>Graficar la evoluci√≥n del aprendizaje.</li>
        <li>Guardar el modelo entrenado (Q-table).</li>
    </ul>

    <h4>3. Interfaz Web Flask</h4>
    <ul>
        <li>Agregar men√∫ con dos secciones: Conceptos y Caso pr√°ctico.</li>
        <li>Mostrar teor√≠a completa y referencias APA.</li>
        <li>Permitir entrenamiento interactivo del agente.</li>
        <li>Visualizar gr√°ficas, m√©tricas y trayectoria.</li>
    </ul>

    <h4>4. GitHub y flujo de trabajo</h4>
    <ul>
        <li>Crear rama <code>A12_ReinforcementLearning</code>.</li>
        <li>Documentar funciones y par√°metros.</li>
        <li>Realizar commits at√≥micos y descriptivos.</li>
        <li>Abrir Pull Request hacia la rama principal.</li>
        <li>Agregar README con entorno, algoritmo y resultados obtenidos.</li>
    </ul>
</div>

<!-- TEOR√çA PRINCIPAL -->
<div class="card-custom">

    <h2>üìò S√≠ntesis Te√≥rica del Aprendizaje por Refuerzo</h2>

    <h3>1. Definici√≥n general</h3>
    <p>
        El Aprendizaje por Refuerzo (Reinforcement Learning, RL) es un paradigma del aprendizaje autom√°tico
        en el que un agente aprende a tomar decisiones mediante interacci√≥n con un entorno.
        En cada paso, el agente observa un estado, ejecuta una acci√≥n, recibe una recompensa
        y transiciona a un nuevo estado. El objetivo es aprender una pol√≠tica que maximice
        la suma de recompensas esperadas a largo plazo.
    </p>

    <h3>2. Comparaci√≥n con Aprendizaje Supervisado y No Supervisado</h3>
    <ul>
        <li><b>Supervisado:</b> aprende a partir de ejemplos etiquetados.</li>
        <li><b>No supervisado:</b> encuentra patrones sin etiquetas.</li>
        <li><b>Refuerzo:</b> no recibe la respuesta correcta; aprende por ensayo y error mediante recompensas.</li>
    </ul>

    <h3>3. Componentes del modelo RL</h3>
    <ul>
        <li><b>Agente:</b> quien aprende y decide.</li>
        <li><b>Entorno (Environment):</b> el mundo donde act√∫a el agente.</li>
        <li><b>Estados (S):</b> representan la situaci√≥n actual.</li>
        <li><b>Acciones (A):</b> decisiones posibles del agente.</li>
        <li><b>Recompensa (R):</b> se√±al num√©rica evaluando la acci√≥n tomada.</li>
        <li><b>Pol√≠tica (œÄ):</b> funci√≥n que mapea estados a acciones.</li>
    </ul>

    <h3>4. Principios del ciclo de aprendizaje</h3>

    <h4>Exploraci√≥n vs. Explotaci√≥n</h4>
    <p>
        La estrategia Œµ-greedy define cu√°ndo explorar nuevas acciones (probabilidad Œµ)
        y cu√°ndo explotar el conocimiento actual (probabilidad 1‚àíŒµ).
    </p>

    <h4>Retorno acumulado</h4>
    <p>
        El retorno es la suma descontada de recompensas:
    </p>

    <code>G‚Çú = Œ£ Œ≥·µè R‚Çú‚Çä‚Çñ‚Çä‚ÇÅ</code>

    <h4>Descuento temporal (Œ≥)</h4>
    <p>
        Controla cu√°nto valor tienen las recompensas futuras (0 ‚â§ Œ≥ ‚â§ 1).
    </p>

    <h3>5. Algoritmos principales</h3>

    <h4>Q-Learning (off-policy)</h4>
    <p>
        Aprende la funci√≥n Q(s,a) maximizando el valor futuro esperado.  
        Actualizaci√≥n:
    </p>

    <code>Q(s,a) ‚Üê Q(s,a) + Œ± [ r + Œ≥ max Q(s',a') ‚àí Q(s,a) ]</code>

    <h4>SARSA (on-policy)</h4>
    <p>
        Actualiza usando la acci√≥n que realmente se tom√≥. M√°s conservador en entornos ruidosos.
    </p>

    <h4>DQN (Deep Q-Network)</h4>
    <p>
        Utiliza redes neuronales para aproximar Q(s,a) en espacios complejos.  
        Fue usado para jugar Atari con nivel humano.
    </p>

    <h3>6. Buenas pr√°cticas</h3>
    <ul>
        <li>Ajustar correctamente Œ±, Œ≥ y Œµ.</li>
        <li>Disminuir Œµ progresivamente.</li>
        <li>Dise√±ar recompensas coherentes.</li>
        <li>Monitorear convergencia mediante promedios m√≥viles.</li>
        <li>En deep RL: usar experience replay y target networks.</li>
    </ul>

    <h3>üìö Referencias APA 7</h3>
    <ul>
        <li>Fran√ßois-Lavet, V., Henderson, P., Islam, R., Bellemare, M. G., & Pineau, J. (2018). <i>An introduction to deep reinforcement learning.</i> Foundations and Trends in Machine Learning.</li>
        <li>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., et al. (2015). <i>Human-level control through deep reinforcement learning.</i> Nature.</li>
        <li>Sutton, R. S., & Barto, A. G. (2018). <i>Reinforcement learning: An introduction</i> (2nd ed.). MIT Press.</li>
    </ul>

</div>

{% endblock %}
